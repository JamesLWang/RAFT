{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d84ad1-cf86-45ce-a61e-0566f0f7f866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd03c9c2a3e04145a9e8453387a12ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kalpeshk2011/dipper-paraphraser-xxl model loaded in 167.92706298828125\n",
      "Input = In a shocking finding, scientist discovered a herd of unicorns living in a remote valley. <sent> They have never been known to mingle with humans. Today, it is believed these unicorns live in an unspoilt environment which is surrounded by mountains. Its edge is protected by a thick wattle of wattle trees, giving it a majestic appearance. Along with their so-called miracle of multicolored coat, their golden coloured feather makes them look like mirages. Some of them are rumored to be capable of speaking a large amount of different languages. They feed on elk and goats as they were selected from those animals that possess a fierceness to them, and can \"eat\" them with their long horns. </sent>\n",
      "\n",
      "Output (Lexical diversity = 60, Sample p = 0.75) =  Their life is said to be unsullied, and they have never been known to mix with humans. It is believed that they live in an isolated and unspoiled valley. The valley is surrounded by mountains, and its edge is thickly protected by a belt of thickets of wattle. Along with their multicoloured, mysterious hair, their golden-hued horns give them the look of some mirage. Some of them, it is rumoured, have the ability to speak many languages. They are believed to feed on elk and goats. They have chosen the animals that are fiercest and can ‘chomp’ them with their long horns.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import time\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class DipperParaphraser(object):\n",
    "    def __init__(self, model=\"kalpeshk2011/dipper-paraphraser-xxl\", verbose=True):\n",
    "        time1 = time.time()\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained('google/t5-v1_1-xxl')\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model, torch_dtype=torch.float16)\n",
    "        if verbose:\n",
    "            print(f\"{model} model loaded in {time.time() - time1}\")\n",
    "        self.model.cuda()\n",
    "        self.model.eval()\n",
    "\n",
    "    def paraphrase(self, input_text, lex_diversity, order_diversity, prefix=\"\", sent_interval=3, **kwargs):\n",
    "        \"\"\"Paraphrase a text using the DIPPER model.\n",
    "\n",
    "        Args:\n",
    "            input_text (str): The text to paraphrase. Make sure to mark the sentence to be paraphrased between <sent> and </sent> blocks, keeping space on either side.\n",
    "            lex_diversity (int): The lexical diversity of the output, choose multiples of 20 from 0 to 100. 0 means no diversity, 100 means maximum diversity.\n",
    "            order_diversity (int): The order diversity of the output, choose multiples of 20 from 0 to 100. 0 means no diversity, 100 means maximum diversity.\n",
    "            **kwargs: Additional keyword arguments like top_p, top_k, max_length.\n",
    "        \"\"\"\n",
    "        assert lex_diversity in [0, 20, 40, 60, 80, 100], \"Lexical diversity must be one of 0, 20, 40, 60, 80, 100.\"\n",
    "        assert order_diversity in [0, 20, 40, 60, 80, 100], \"Order diversity must be one of 0, 20, 40, 60, 80, 100.\"\n",
    "\n",
    "        lex_code = int(100 - lex_diversity)\n",
    "        order_code = int(100 - order_diversity)\n",
    "\n",
    "        input_text = \" \".join(input_text.split())\n",
    "        sentences = sent_tokenize(input_text)\n",
    "        prefix = \" \".join(prefix.replace(\"\\n\", \" \").split())\n",
    "        output_text = \"\"\n",
    "\n",
    "        for sent_idx in range(0, len(sentences), sent_interval):\n",
    "            curr_sent_window = \" \".join(sentences[sent_idx:sent_idx + sent_interval])\n",
    "            final_input_text = f\"lexical = {lex_code}, order = {order_code}\"\n",
    "            if prefix:\n",
    "                final_input_text += f\" {prefix}\"\n",
    "            final_input_text += f\" <sent> {curr_sent_window} </sent>\"\n",
    "\n",
    "            final_input = self.tokenizer([final_input_text], return_tensors=\"pt\")\n",
    "            final_input = {k: v.cuda() for k, v in final_input.items()}\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                outputs = self.model.generate(**final_input, **kwargs)\n",
    "            outputs = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            prefix += \" \" + outputs[0]\n",
    "            output_text += \" \" + outputs[0]\n",
    "\n",
    "        return output_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dp = DipperParaphraser()\n",
    "\n",
    "    prompt = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote valley.\"\n",
    "    input_text = \"They have never been known to mingle with humans. Today, it is believed these unicorns live in an unspoilt environment which is surrounded by mountains. Its edge is protected by a thick wattle of wattle trees, giving it a majestic appearance. Along with their so-called miracle of multicolored coat, their golden coloured feather makes them look like mirages. Some of them are rumored to be capable of speaking a large amount of different languages. They feed on elk and goats as they were selected from those animals that possess a fierceness to them, and can \\\"eat\\\" them with their long horns.\"\n",
    "\n",
    "    print(f\"Input = {prompt} <sent> {input_text} </sent>\\n\")\n",
    "    output_l60_sample = dp.paraphrase(input_text, lex_diversity=60, order_diversity=0, prefix=prompt, do_sample=True, top_p=0.75, top_k=None, max_length=512)\n",
    "    print(f\"Output (Lexical diversity = 60, Sample p = 0.75) = {output_l60_sample}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348b8678-0aa3-4828-946b-47fe2664d1da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
    "        \n",
    "        \n",
    "datasets = {\n",
    "    'abstract': load_json_file(\"./datasets/abstract/abstract_gpt-3.5-turbo.raw_data.json\"),\n",
    "    'squad': load_json_file(\"./datasets/squad/squad_gpt-3.5-turbo.raw_data.json\"),\n",
    "    'xsum': load_json_file(\"./datasets/xsum/xsum_gpt-3.5-turbo.raw_data.json\"),\n",
    "    'writing': load_json_file(\"./datasets/writing/writing_gpt-3.5-turbo.raw_data.json\") \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1dbab7-65d8-48ab-aa5f-82620d54b991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "350it [17:00,  2.91s/it]\n",
      "200it [18:00,  5.40s/it]\n",
      "150it [17:30,  7.01s/it]\n",
      "150it [19:06,  7.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    datasets[dataset_name]['evade'] = []\n",
    "    \n",
    "    save_path = os.path.join(f'./datasets/{dataset_name}/', f'{dataset_name}_evasion_dipper.json')\n",
    "    \n",
    "    for i,data in tqdm(enumerate(dataset['sampled'])):\n",
    "        data_evade = dp.paraphrase(data, lex_diversity=20, order_diversity=60, prefix=prompt, do_sample=True, top_p=0.75, top_k=None, max_length=512)\n",
    "        datasets[dataset_name]['evade'].append(data_evade)\n",
    "        \n",
    "        with open(save_path, 'w') as output_file:\n",
    "            json.dump(datasets[dataset_name], output_file)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c8f72-aa9a-47e9-8757-6f889720b27e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
